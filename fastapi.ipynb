{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c952b70-1340-4829-b06e-ef659e717f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/fish-speech\n",
      "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/root/autodl-tmp/fish-speech']\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m5457\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
      "\u001b[32m12-19 22:02:03\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:222 | FastAPI, 启动!\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m5459\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:35478 - \"\u001b[1mPOST /del-model HTTP/1.1\u001b[0m\" \u001b[31m422 Unprocessable Entity\u001b[0m\n",
      "\u001b[33mWARNING\u001b[0m:  StatReload detected changes in 'server.py'. Reloading...\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m5459\u001b[0m]\n",
      "\u001b[32m12-19 22:03:48\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:222 | FastAPI, 启动!\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m5533\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32m12-19 22:03:56\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:405 | 127.0.0.1:51974/del-model   \n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51974 - \"\u001b[1mPOST /del-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:04:06\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:405 | 127.0.0.1:45718/del-model   \n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:45718 - \"\u001b[1mPOST /del-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:04:09\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:247 | 127.0.0.1:45724/load-model   config_name=text2semantic_finetune\n",
      "\u001b[32m12-19 22:04:09\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:251 | Loading model ...\n",
      "\u001b[32m12-19 22:04:11\u001b[0m \u001b[1mINFO     \u001b[0m| generate.py:358 | Restored model from checkpoint\n",
      "\u001b[32m12-19 22:04:11\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:46 | Time to load model: 2.17 seconds\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:45724 - \"\u001b[1mPOST /load-llama-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:04:26\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:247 | 127.0.0.1:33618/load-model   config_name=text2semantic_finetune\n",
      "\u001b[32m12-19 22:04:26\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:251 | Loading model ...\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:33618 - \"\u001b[1mPOST /load-llama-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:04:28\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:247 | 127.0.0.1:33628/load-model   config_name=text2semantic_finetune\n",
      "\u001b[32m12-19 22:04:28\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:251 | Loading model ...\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:33628 - \"\u001b[1mPOST /load-llama-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:04:29\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:247 | 127.0.0.1:33630/load-model   config_name=text2semantic_finetune\n",
      "\u001b[32m12-19 22:04:29\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:251 | Loading model ...\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:33630 - \"\u001b[1mPOST /load-llama-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:04:30\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:272 | 127.0.0.1:33634/load-model   config_name=vqgan_pretrain\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "\u001b[32m12-19 22:04:32\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:108 | Restored model from checkpoint\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:33634 - \"\u001b[1mPOST /load-vqgan-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:04:33\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:272 | 127.0.0.1:56024/load-model   config_name=vqgan_pretrain\n",
      "\u001b[32m12-19 22:04:33\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:108 | Restored model from checkpoint\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:56024 - \"\u001b[1mPOST /load-vqgan-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:04:34\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:272 | 127.0.0.1:56028/load-model   config_name=vqgan_pretrain\n",
      "\u001b[32m12-19 22:04:34\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:108 | Restored model from checkpoint\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:56028 - \"\u001b[1mPOST /load-vqgan-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:04:35\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:272 | 127.0.0.1:56030/load-model   config_name=vqgan_pretrain\n",
      "\u001b[32m12-19 22:04:35\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:108 | Restored model from checkpoint\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:56030 - \"\u001b[1mPOST /load-vqgan-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:05:41\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:306 | 127.0.0.1:36336/use-model   \n",
      "\u001b[32m12-19 22:05:41\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:343 | Encoded prompt shape: torch.Size([5, 121])\n",
      "  6%|██▌                                      | 64/1023 [00:01<00:22, 42.11it/s]\n",
      "\u001b[32m12-19 22:05:43\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:363 | Compilation time: 2.04 seconds\n",
      "\u001b[32m12-19 22:05:43\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 66 tokens in 2.04 seconds, 32.41 tokens/sec\n",
      "\u001b[32m12-19 22:05:43\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 12.46 GB/s\n",
      "\u001b[32m12-19 22:05:43\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.25 GB\n",
      "\u001b[32m12-19 22:05:43\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_0.npy\n",
      "\u001b[32m12-19 22:05:43\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_0.npy\n",
      "\u001b[32m12-19 22:05:44\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 65, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:05:44\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 66560]), equivalent to 3.02 seconds\n",
      "\u001b[32m12-19 22:05:44\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_0.wav\n",
      "  6%|██▍                                      | 61/1023 [00:01<00:24, 38.77it/s]\n",
      "\u001b[32m12-19 22:05:46\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 63 tokens in 1.61 seconds, 39.20 tokens/sec\n",
      "\u001b[32m12-19 22:05:46\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 15.06 GB/s\n",
      "\u001b[32m12-19 22:05:46\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.39 GB\n",
      "\u001b[32m12-19 22:05:46\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_1.npy\n",
      "\u001b[32m12-19 22:05:46\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_1.npy\n",
      "\u001b[32m12-19 22:05:46\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 62, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:05:46\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 63488]), equivalent to 2.88 seconds\n",
      "\u001b[32m12-19 22:05:46\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_1.wav\n",
      "  7%|██▊                                      | 71/1023 [00:01<00:19, 48.66it/s]\n",
      "\u001b[32m12-19 22:05:48\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 73 tokens in 1.49 seconds, 49.07 tokens/sec\n",
      "\u001b[32m12-19 22:05:48\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 18.86 GB/s\n",
      "\u001b[32m12-19 22:05:48\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.39 GB\n",
      "\u001b[32m12-19 22:05:48\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_2.npy\n",
      "\u001b[32m12-19 22:05:48\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_2.npy\n",
      "\u001b[32m12-19 22:05:48\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 72, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:05:48\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 73728]), equivalent to 3.34 seconds\n",
      "\u001b[32m12-19 22:05:48\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_2.wav\n",
      "  8%|███▏                                     | 81/1023 [00:01<00:22, 41.48it/s]\n",
      "\u001b[32m12-19 22:05:50\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 83 tokens in 1.98 seconds, 41.84 tokens/sec\n",
      "\u001b[32m12-19 22:05:50\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 16.08 GB/s\n",
      "\u001b[32m12-19 22:05:50\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.39 GB\n",
      "\u001b[32m12-19 22:05:50\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_3.npy\n",
      "\u001b[32m12-19 22:05:50\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_3.npy\n",
      "\u001b[32m12-19 22:05:50\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 82, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:05:50\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 83968]), equivalent to 3.81 seconds\n",
      "\u001b[32m12-19 22:05:50\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_3.wav\n",
      "  5%|██                                       | 53/1023 [00:01<00:24, 38.85it/s]\n",
      "\u001b[32m12-19 22:05:52\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 55 tokens in 1.40 seconds, 39.34 tokens/sec\n",
      "\u001b[32m12-19 22:05:52\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 15.12 GB/s\n",
      "\u001b[32m12-19 22:05:52\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.39 GB\n",
      "\u001b[32m12-19 22:05:52\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_4.npy\n",
      "\u001b[32m12-19 22:05:52\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_4.npy\n",
      "\u001b[32m12-19 22:05:52\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 54, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:05:52\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 55296]), equivalent to 2.51 seconds\n",
      "\u001b[32m12-19 22:05:52\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_4.wav\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:36336 - \"\u001b[1mPOST /use-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32m12-19 22:06:09\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:306 | 127.0.0.1:43636/use-model   \n",
      "\u001b[32m12-19 22:06:09\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:343 | Encoded prompt shape: torch.Size([5, 290])\n",
      " 49%|███████████████████▋                    | 502/1023 [00:10<00:10, 49.84it/s]\n",
      "\u001b[32m12-19 22:06:20\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:363 | Compilation time: 10.10 seconds\n",
      "\u001b[32m12-19 22:06:20\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 504 tokens in 10.10 seconds, 49.90 tokens/sec\n",
      "\u001b[32m12-19 22:06:20\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 19.17 GB/s\n",
      "\u001b[32m12-19 22:06:20\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.39 GB\n",
      "\u001b[32m12-19 22:06:20\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_0.npy\n",
      "\u001b[32m12-19 22:06:20\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_0.npy\n",
      "\u001b[32m12-19 22:06:20\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 503, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:06:20\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 515072]), equivalent to 23.36 seconds\n",
      "\u001b[32m12-19 22:06:20\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_0.wav\n",
      " 50%|████████████████████                    | 514/1023 [00:10<00:10, 48.56it/s]\n",
      "\u001b[32m12-19 22:06:30\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 516 tokens in 10.62 seconds, 48.59 tokens/sec\n",
      "\u001b[32m12-19 22:06:30\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 18.67 GB/s\n",
      "\u001b[32m12-19 22:06:30\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.58 GB\n",
      "\u001b[32m12-19 22:06:30\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_1.npy\n",
      "\u001b[32m12-19 22:06:30\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_1.npy\n",
      "\u001b[32m12-19 22:06:30\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 515, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:06:30\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 527360]), equivalent to 23.92 seconds\n",
      "\u001b[32m12-19 22:06:30\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_1.wav\n",
      " 55%|██████████████████████▏                 | 567/1023 [00:11<00:09, 49.66it/s]\n",
      "\u001b[32m12-19 22:06:42\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 569 tokens in 11.45 seconds, 49.68 tokens/sec\n",
      "\u001b[32m12-19 22:06:42\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 19.09 GB/s\n",
      "\u001b[32m12-19 22:06:42\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.65 GB\n",
      "\u001b[32m12-19 22:06:42\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_2.npy\n",
      "\u001b[32m12-19 22:06:42\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_2.npy\n",
      "\u001b[32m12-19 22:06:42\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 568, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:06:42\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 581632]), equivalent to 26.38 seconds\n",
      "\u001b[32m12-19 22:06:42\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_2.wav\n",
      " 53%|█████████████████████▎                  | 546/1023 [00:11<00:09, 48.02it/s]\n",
      "\u001b[32m12-19 22:06:54\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 548 tokens in 11.40 seconds, 48.07 tokens/sec\n",
      "\u001b[32m12-19 22:06:54\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 18.47 GB/s\n",
      "\u001b[32m12-19 22:06:54\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.73 GB\n",
      "\u001b[32m12-19 22:06:54\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_3.npy\n",
      "\u001b[32m12-19 22:06:54\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_3.npy\n",
      "\u001b[32m12-19 22:06:54\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 547, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:06:54\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 560128]), equivalent to 25.40 seconds\n",
      "\u001b[32m12-19 22:06:54\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_3.wav\n",
      " 57%|██████████████████████▊                 | 585/1023 [00:12<00:09, 48.49it/s]\n",
      "\u001b[32m12-19 22:07:06\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:370 | Generated 587 tokens in 12.10 seconds, 48.51 tokens/sec\n",
      "\u001b[32m12-19 22:07:06\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:373 | Bandwidth achieved: 18.64 GB/s\n",
      "\u001b[32m12-19 22:07:06\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:374 | GPU Memory used: 1.73 GB\n",
      "\u001b[32m12-19 22:07:06\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:383 | Saved codes to codes_4.npy\n",
      "\u001b[32m12-19 22:07:06\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:177 | Processing precomputed indices from codes_4.npy\n",
      "\u001b[32m12-19 22:07:06\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:196 | VQ Encoded, indices: torch.Size([4, 1, 586, 1]) equivalent to 21.53 Hz\n",
      "\u001b[32m12-19 22:07:06\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:206 | Generated audio of shape torch.Size([1, 1, 600064]), equivalent to 27.21 seconds\n",
      "\u001b[32m12-19 22:07:06\u001b[0m \u001b[1mINFO     \u001b[0m| server.py:213 | Saved audio to fake_4.wav\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:43636 - \"\u001b[1mPOST /use-model HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd ~/autodl-tmp/fish-speech\n",
    "!uvicorn server:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59caed2-f884-4a9e-b4de-9fb58f38b9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
